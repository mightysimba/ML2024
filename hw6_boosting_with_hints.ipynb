{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMms33J4jeDZ"
      },
      "source": [
        "1. Use the same dataset from the previous task\n",
        "2. Reuse validation strategy and preprocessing without changes\n",
        "3. Train xgboost model\n",
        "4. Train lightgbm model\n",
        "5. Train catboost model\n",
        "6. Compare performance on local validation and on test set on kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load train data\n",
        "# reuse the preprocessing approach from the previous homework\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from collections import Counter\n",
        "from sklearn.base import clone\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train_df = pd.read_csv('C:\\\\Users\\\\FILMINVASION\\\\Downloads\\\\ML2024\\\\train.csv')\n",
        "test_df = pd.read_csv('C:\\\\Users\\\\FILMINVASION\\\\Downloads\\\\ML2024\\\\test.csv')\n",
        "\n",
        "passenger_ids = test_df['PassengerId'].copy()\n",
        "\n",
        "def preprocess_data(train_df, test_df):\n",
        "    \n",
        "    # Зберігаємо статистичні значення з тренувального набору даних\n",
        "    age_mean = train_df['Age'].mean()\n",
        "    embarked_mode = train_df['Embarked'].mode()[0]\n",
        "\n",
        "    # Заповнюємо пропуски у тренувальному та тестовому наборі однаковими значеннями\n",
        "    train_df['Age'] = train_df['Age'].fillna(age_mean)\n",
        "    test_df['Age'] = test_df['Age'].fillna(age_mean)\n",
        "\n",
        "    train_df['Embarked'] = train_df['Embarked'].fillna(embarked_mode)\n",
        "    test_df['Embarked'] = test_df['Embarked'].fillna(embarked_mode)\n",
        "\n",
        "    # Drop unnecessary columns that are not useful for modeling\n",
        "    train_df = train_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
        "    test_df = test_df.drop([ 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "    # Застосовуємо one-hot encoding до обох наборів даних\n",
        "    train_df = pd.get_dummies(train_df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "    test_df = pd.get_dummies(test_df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Call this function before splitting data\n",
        "train_df, test_df = preprocess_data(train_df, test_df)\n",
        "\n",
        "X = train_df.drop('Survived', axis=1)  # Features\n",
        "y = train_df['Survived']  # Target\n",
        "\n",
        "n_splits = 5\n",
        "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for train_index, val_index in stratified_kfold.split(X, y):\n",
        "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost hyperparameters (initially set)\n",
        "xgb_params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',  # You can also use 'error' or other metrics\n",
        "    'use_label_encoder': False,\n",
        "    'n_estimators': 1000,  # Set high and allow early stopping to adjust\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 3,  # Can tune further\n",
        "    'subsample': 0.8,  # Can tune further\n",
        "    'colsample_bytree': 0.8,  # Can tune further\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Loop over each fold\n",
        "best_iteration_per_fold = []\n",
        "accuracy_per_fold = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(stratified_kfold.split(X, y)):\n",
        "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Create an XGBoost classifier with the defined hyperparameters\n",
        "    xgb_model = xgb.XGBClassifier(**xgb_params)\n",
        "    \n",
        "    # Fit the model with early stopping\n",
        "    xgb_model.fit(\n",
        "        X_train_fold, y_train_fold,\n",
        "        eval_set=[(X_val_fold, y_val_fold)],\n",
        "        early_stopping_rounds=10,  # Stop after 10 rounds without improvement\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    # Store the best iteration (number of rounds)\n",
        "    best_iteration_per_fold.append(xgb_model.best_iteration)\n",
        "    \n",
        "    # Predict on validation set and compute accuracy\n",
        "    y_pred_val = xgb_model.predict(X_val_fold)\n",
        "    accuracy = accuracy_score(y_val_fold, y_pred_val)\n",
        "    accuracy_per_fold.append(accuracy)\n",
        "    \n",
        "    print(f\"Fold {fold+1} - Best Iteration: {xgb_model.best_iteration}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate the average best number of boosting rounds and accuracy\n",
        "average_best_iteration = int(np.mean(best_iteration_per_fold))\n",
        "average_accuracy = np.mean(accuracy_per_fold)\n",
        "\n",
        "print(f\"Average Best Iteration: {average_best_iteration}\")\n",
        "print(f\"Average Accuracy: {average_accuracy:.4f}\")\n",
        "\n",
        "# Retrain the model on the full training data using the best number of boosting rounds\n",
        "xgb_final_model = xgb.XGBClassifier(\n",
        "    **xgb_params,\n",
        "    n_estimators=average_best_iteration  # Use the average best boosting rounds\n",
        ")\n",
        "\n",
        "xgb_final_model.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 - Best Iteration: 89, Accuracy: 0.8603\n",
            "Fold 2 - Best Iteration: 33, Accuracy: 0.8202\n",
            "Fold 3 - Best Iteration: 37, Accuracy: 0.8034\n",
            "Fold 4 - Best Iteration: 39, Accuracy: 0.7921\n",
            "Fold 5 - Best Iteration: 57, Accuracy: 0.8315\n",
            "Average Best Iteration: 51\n",
            "Average Accuracy: 0.8215\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-12 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-12 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-12 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-12 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-12 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-12 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-12 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-12 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-12 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-12 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-12 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-12 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-12 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-12 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-12 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-12 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
              "              feature_types=None, gamma=None, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
              "              max_leaves=None, min_child_weight=None, missing=nan,\n",
              "              monotone_constraints=None, multi_strategy=None, n_estimators=51,\n",
              "              n_jobs=None, num_parallel_tree=None, random_state=32, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
              "              feature_types=None, gamma=None, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
              "              max_leaves=None, min_child_weight=None, missing=nan,\n",
              "              monotone_constraints=None, multi_strategy=None, n_estimators=51,\n",
              "              n_jobs=None, num_parallel_tree=None, random_state=32, ...)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric='logloss',\n",
              "              feature_types=None, gamma=None, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
              "              max_leaves=None, min_child_weight=None, missing=nan,\n",
              "              monotone_constraints=None, multi_strategy=None, n_estimators=51,\n",
              "              n_jobs=None, num_parallel_tree=None, random_state=32, ...)"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define the xgboost model (from xgboost package)\n",
        "# You can now use xgb_final_model for making predictions or further evaluations\n",
        "# define the hyperparameters\n",
        "# train the model\n",
        "# try to improve the model by changing the hyperparameters on local validation (remember that using gridsearch is a bad idea, because it can't use the early stopping)\n",
        "# retrain the model on the whole train dataset\n",
        "# don't forget to specify the number of boosting rounds you found optimal\n",
        "\n",
        "\n",
        "xgb_params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 3,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': 32\n",
        "}\n",
        "\n",
        "# Stratified K-Fold\n",
        "n_splits = 5\n",
        "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "best_iteration_per_fold = []\n",
        "accuracy_per_fold = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(stratified_kfold.split(X, y)):\n",
        "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Create an XGBoost classifier with the defined hyperparameters\n",
        "    xgb_model = xgb.XGBClassifier(**xgb_params, early_stopping_rounds=10)\n",
        "    \n",
        "    # Fit the model with early stopping\n",
        "    xgb_model.fit(\n",
        "    X_train_fold, y_train_fold,\n",
        "    eval_set=[(X_val_fold, y_val_fold)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "    \n",
        "    # Store the best iteration (number of boosting rounds)\n",
        "    best_iteration_per_fold.append(xgb_model.best_iteration)\n",
        "    \n",
        "    # Predict on validation set and compute accuracy\n",
        "    y_pred_val = xgb_model.predict(X_val_fold)\n",
        "    accuracy = accuracy_score(y_val_fold, y_pred_val)\n",
        "    accuracy_per_fold.append(accuracy)\n",
        "    \n",
        "    print(f\"Fold {fold+1} - Best Iteration: {xgb_model.best_iteration}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate the average best number of boosting rounds and accuracy\n",
        "average_best_iteration_xgb = int(np.mean(best_iteration_per_fold))\n",
        "average_accuracy_xgb = np.mean(accuracy_per_fold)\n",
        "\n",
        "print(f\"Average Best Iteration: {average_best_iteration_xgb}\")\n",
        "print(f\"Average Accuracy: {average_accuracy_xgb:.4f}\")\n",
        "\n",
        "# Retrain the model on the full training data using the best number of boosting rounds\n",
        "xgb_final_model = xgb.XGBClassifier(\n",
        "    **xgb_params,\n",
        "    n_estimators=average_best_iteration_xgb\n",
        ")\n",
        "\n",
        "# Fit the final model on the entire dataset\n",
        "xgb_final_model.fit(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\ttrain-logloss:0.61716+0.00165\ttest-logloss:0.62181+0.00447\n",
            "[10]\ttrain-logloss:0.44317+0.00669\ttest-logloss:0.48967+0.02211\n",
            "[20]\ttrain-logloss:0.36486+0.00743\ttest-logloss:0.44336+0.03109\n",
            "[30]\ttrain-logloss:0.31951+0.00784\ttest-logloss:0.42889+0.04027\n",
            "[40]\ttrain-logloss:0.28866+0.00775\ttest-logloss:0.42836+0.04577\n",
            "[44]\ttrain-logloss:0.27856+0.00723\ttest-logloss:0.43074+0.04731\n",
            "Best Iteration (from CV): 34\n",
            "Final Model Accuracy on Test Data: 0.8799\n"
          ]
        }
      ],
      "source": [
        "#Перероблений варіант\n",
        "\n",
        "xgb_params = {\n",
        "      #default\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eta\": 0.1,\n",
        "    \"verbosity\": 0,\n",
        "    \"nthread\": 10,\n",
        "    \"random_seed\": 1,\n",
        "    \"eval_metric\": \"logloss\",\n",
        "\n",
        "    # regularization parameters\n",
        "    \"max_depth\": 5,\n",
        "    \"subsample\": 0.7,\n",
        "    \"colsample_bytree\": 0.7\n",
        "}\n",
        "\n",
        "# Stratified K-Fold\n",
        "n_splits = 5\n",
        "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Convert data to DMatrix\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "\n",
        "# Perform cross-validation using xgb.cv\n",
        "cv_results = xgb.cv(\n",
        "    params=xgb_params,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=1000,\n",
        "    nfold=n_splits,\n",
        "    stratified=True,\n",
        "    folds=stratified_kfold,\n",
        "    early_stopping_rounds=10,\n",
        "    verbose_eval=10\n",
        ")\n",
        "\n",
        "# Extract the best number of boosting rounds\n",
        "best_iteration_xgb = cv_results['test-logloss-mean'].idxmin()\n",
        "\n",
        "print(f\"Best Iteration (from CV): {best_iteration_xgb}\")\n",
        "\n",
        "# Retrain the model on the full training data using the best number of boosting rounds\n",
        "xgb_final_model = xgb.XGBClassifier(\n",
        "    **xgb_params,\n",
        "    n_estimators=best_iteration_xgb\n",
        ")\n",
        "\n",
        "# Fit the final model on the entire dataset\n",
        "xgb_final_model.fit(X, y)\n",
        "\n",
        "# Now you can make predictions or further evaluate the model\n",
        "y_pred = xgb_final_model.predict(X)\n",
        "accuracy_xgb = accuracy_score(y, y_pred)\n",
        "\n",
        "print(f\"Final Model Accuracy on Test Data: {accuracy_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "ZSixk8wXjZJZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 - Best Iteration: 60, Accuracy: 0.8380\n",
            "Fold 2 - Best Iteration: 103, Accuracy: 0.8202\n",
            "Fold 3 - Best Iteration: 44, Accuracy: 0.8146\n",
            "Fold 4 - Best Iteration: 46, Accuracy: 0.8146\n",
            "Fold 5 - Best Iteration: 33, Accuracy: 0.8315\n",
            "Average Best Iteration: 57\n",
            "Average Accuracy: 0.8238\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "# define the lightgbm model (from lightgbm package)\n",
        "# define the hyperparameters\n",
        "# train the model\n",
        "# try to improve the model by changing the hyperparameters on local validation (remember that using gridsearch is a bad idea, because it can't use the early stopping)\n",
        "# retrain the model on the whole train dataset\n",
        "# don't forget to specify the number of boosting rounds you found optimal\n",
        "# Set up hyperparameters\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 3,\n",
        "    'num_leaves': 31,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': 42,\n",
        "    \"early_stopping_rounds\": 10,\n",
        "    'verbosity' : -1\n",
        "    }\n",
        "\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "best_iteration_per_fold = []\n",
        "accuracy_per_fold = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(stratified_kfold.split(X, y)):\n",
        "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Create LightGBM dataset\n",
        "    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
        "    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
        "\n",
        "    # Train model with early stopping\n",
        "    lgb_model = lgb.train(lgb_params, train_data, num_boost_round=1000, valid_sets=[train_data, val_data])\n",
        "\n",
        "    # Store best iteration and accuracy\n",
        "    best_iteration_per_fold.append(lgb_model.best_iteration)\n",
        "    y_pred_val = (lgb_model.predict(X_val_fold, num_iteration=lgb_model.best_iteration) > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_val_fold, y_pred_val)\n",
        "    accuracy_per_fold.append(accuracy)\n",
        "\n",
        "    print(f\"Fold {fold+1} - Best Iteration: {lgb_model.best_iteration}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate the average best boosting rounds and accuracy\n",
        "average_best_iteration_lgb = int(np.mean(best_iteration_per_fold))\n",
        "average_accuracy_lgb = np.mean(accuracy_per_fold)\n",
        "\n",
        "print(f\"Average Best Iteration: {average_best_iteration_lgb}\")\n",
        "print(f\"Average Accuracy: {average_accuracy_lgb:.4f}\")\n",
        "\n",
        "# Retrain the model on the full training data using the best number of boosting rounds\n",
        "final_train_data = lgb.Dataset(X, label=y)\n",
        "lgb_final_model = lgb.train({**lgb_params, 'early_stopping_rounds': None},\n",
        "                            final_train_data, num_boost_round=average_best_iteration_lgb, )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "Early stopping, best iteration is:\n",
            "[48]\tcv_agg's valid binary_logloss: 0.407485 + 0.0177201\n",
            "Best number of boosting rounds from CV: 48\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping\n",
        "# Define the parameters\n",
        "lgb_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 3,\n",
        "    'num_leaves': 31,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': 42,\n",
        "    'verbosity': -1\n",
        "}\n",
        "\n",
        "# Create LightGBM dataset\n",
        "train_data = lgb.Dataset(X, label=y)\n",
        "\n",
        "# Perform cross-validation using lgb.cv\n",
        "cv_results = lgb.cv(\n",
        "    lgb_params,\n",
        "    train_data,\n",
        "    num_boost_round=1000,\n",
        "    nfold=5,\n",
        "    stratified=True,\n",
        "    shuffle=True,\n",
        "    callbacks=[early_stopping(stopping_rounds=10)],\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Best number of boosting rounds\n",
        "best_num_boost_rounds = len(cv_results['valid binary_logloss-mean'])\n",
        "print(f\"Best number of boosting rounds from CV: {best_num_boost_rounds}\")\n",
        "\n",
        "# Train the final model with the optimal number of boosting rounds\n",
        "final_train_data = lgb.Dataset(X, label=y)\n",
        "lgb_final_model = lgb.train(lgb_params, final_train_data, num_boost_round=best_num_boost_rounds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['valid binary_logloss-mean', 'valid binary_logloss-stdv'])\n"
          ]
        }
      ],
      "source": [
        "print(cv_results.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 - Best Iteration: 84, Accuracy: 0.8603\n",
            "Fold 2 - Best Iteration: 46, Accuracy: 0.8258\n",
            "Fold 3 - Best Iteration: 34, Accuracy: 0.8034\n",
            "Fold 4 - Best Iteration: 28, Accuracy: 0.8258\n",
            "Fold 5 - Best Iteration: 28, Accuracy: 0.8315\n",
            "Average Best Iteration: 44\n",
            "Average Accuracy: 0.8294\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x2a9b9c52030>"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "# define the catboost model (from catboost package)\n",
        "# define the hyperparameters\n",
        "# train the model\n",
        "# try to improve the model by changing the hyperparameters on local validation (remember that using gridsearch is a bad idea, because it can't use the early stopping)\n",
        "# retrain the model on the whole train dataset\n",
        "# don't forget to specify the number of boosting rounds you found optimal\n",
        "\n",
        "catboost_params = {\n",
        "    'iterations': 1000, \n",
        "    'depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'eval_metric': 'Logloss',\n",
        "    'random_seed': 42,\n",
        "    'logging_level': 'Silent',\n",
        "    'early_stopping_rounds': 10\n",
        "}\n",
        "\n",
        "# Initialize stratified k-fold cross-validation\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "best_iteration_per_fold = []\n",
        "accuracy_per_fold = []\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold, (train_index, val_index) in enumerate(stratified_kfold.split(X, y)):\n",
        "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "    # Create CatBoost Pool objects\n",
        "    train_data = Pool(data=X_train_fold, label=y_train_fold)\n",
        "    val_data = Pool(data=X_val_fold, label=y_val_fold)\n",
        "\n",
        "    # Train the CatBoost model\n",
        "    catboost_model = CatBoostClassifier(**catboost_params)\n",
        "    catboost_model.fit(train_data, eval_set=val_data, use_best_model=True)\n",
        "\n",
        "    # Store best iteration and accuracy\n",
        "    best_iteration_per_fold.append(catboost_model.best_iteration_)\n",
        "    y_pred_val = catboost_model.predict(X_val_fold)\n",
        "    accuracy = accuracy_score(y_val_fold, y_pred_val)\n",
        "    accuracy_per_fold.append(accuracy)\n",
        "\n",
        "    print(f\"Fold {fold+1} - Best Iteration: {catboost_model.best_iteration_}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate the average best boosting rounds and accuracy\n",
        "average_best_iteration_cat = int(np.mean(best_iteration_per_fold))\n",
        "average_accuracy_cat = np.mean(accuracy_per_fold)\n",
        "\n",
        "print(f\"Average Best Iteration: {average_best_iteration_cat}\")\n",
        "print(f\"Average Accuracy: {average_accuracy_cat:.4f}\")\n",
        "\n",
        "# Retrain the model on the full training data using the best number of boosting rounds\n",
        "final_train_data = Pool(data=X, label=y)\n",
        "final_catboost_model = CatBoostClassifier(**{**catboost_params, 'iterations': average_best_iteration_cat})\n",
        "final_catboost_model.fit(final_train_data)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost:\n",
            "Average best iteration: 51 Average accuracy: 0.8215052413533362\n",
            "Lightgbm:\n",
            "Average best iteration: 57 Average accuracy: 0.8237775406440273\n",
            "Catboost:\n",
            "Average best iteration: 44 Average accuracy: 0.8293704098926622\n",
            "Random Forest result from previous homework:\n",
            "Best Cross-Validation Accuracy: 0.8451007469713137\n"
          ]
        }
      ],
      "source": [
        "# compare the results of the three models from this homework and with models from the previous homework\n",
        "# make a conclusion on which model is better and why\n",
        "# if your boosting is worse than the RF, try to improve it\n",
        "print(\"XGBoost:\\nAverage best iteration:\", average_best_iteration_xgb,\"Average accuracy:\", average_accuracy_xgb)\n",
        "print(\"Lightgbm:\\nAverage best iteration:\", average_best_iteration_lgb, \"Average accuracy:\", average_accuracy_lgb)\n",
        "print(\"Catboost:\\nAverage best iteration:\", average_best_iteration_cat, \"Average accuracy:\", average_accuracy_cat)\n",
        "\n",
        "\n",
        "print(\"Random Forest result from previous homework:\\nBest Cross-Validation Accuracy: 0.8451007469713137\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load test data\n",
        "# do the same preprocessing as for train data\n",
        "\n",
        "# using retrained models make predictions on the test data for all new three models\n",
        "# save the predictions to a file\n",
        "# upload the predictions to Kaggle and make a submission\n",
        "# report the score you got and compare it with the score you got on the validation data\n",
        "# make a conclusion on how well the models generalizes\n",
        "\n",
        "\n",
        "\n",
        "# XGBoost\n",
        "y_pred_xgb = (xgb_final_model.predict(test_df) > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# LightGBM\n",
        "y_pred_lgb = (lgb_final_model.predict(test_df) > 0.5).astype(int)\n",
        "\n",
        "\n",
        "# CatBoost\n",
        "y_pred_catboost = final_catboost_model.predict(test_df).astype(int)\n",
        "\n",
        "\n",
        "# After making predictions, create the submission DataFrame\n",
        "submission_xgb = pd.DataFrame({\n",
        "    'PassengerId': passenger_ids,\n",
        "    'Survived': y_pred_xgb \n",
        "})\n",
        "\n",
        "submission_lgb = pd.DataFrame({\n",
        "    'PassengerId': passenger_ids,\n",
        "    'Survived': y_pred_lgb\n",
        "})\n",
        "\n",
        "submission_cat = pd.DataFrame({\n",
        "    'PassengerId': passenger_ids,\n",
        "    'Survived': y_pred_catboost\n",
        "})\n",
        "# Save the submission file\n",
        "submission_xgb.to_csv('submission_xgb.csv', index=False)\n",
        "submission_lgb.to_csv('submission_lgb.csv', index=False)\n",
        "submission_cat.to_csv('submission_cat.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission to Kaggle for XGBoost score is: 0.75837\n",
            "Submission to Kaggle for LightGBM score is: 0.77990\n",
            "Submission to Kaggle for CatBoost score is: 0.77272\n"
          ]
        }
      ],
      "source": [
        "print(\"Submission to Kaggle for XGBoost score is: 0.75837\")\n",
        "print(\"Submission to Kaggle for LightGBM score is: 0.77990\")\n",
        "print(\"Submission to Kaggle for CatBoost score is: 0.77272\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Bagging homework.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
