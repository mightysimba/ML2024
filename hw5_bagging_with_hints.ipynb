{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMms33J4jeDZ"
      },
      "source": [
        "1. Use any binary classification dataset\n",
        "2. Define validation strategy and use it for all next steps without changes\n",
        "3. Train decision tree model and estimate performance on validation\n",
        "4. Train bagging model with decision tree as a base model and estimate performance on validation\n",
        "5. Write your own bagging implementation:\n",
        "  <br>5.1. Define init for our CustomBaggingClassifier\n",
        "  <br>5.2. Write fit as described in lecture: divide train data on n parts (`n_estimators` in CustomBaggingClassifier), train `base_estimator` on each part and save these models inside class\n",
        "  <br>5.3. For predictions we should use all saved models and combine their predictions (as voting)\n",
        "6. Compare performance of sklearn bagging model with your own implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.base import clone\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train_df = pd.read_csv('C:\\\\Users\\\\FILMINVASION\\\\Downloads\\\\ML2024\\\\train.csv')\n",
        "test_df = pd.read_csv('C:\\\\Users\\\\FILMINVASION\\\\Downloads\\\\ML2024\\\\test.csv')\n",
        "\n",
        "\n",
        "# load train data\n",
        "# reuse the preprocessing approach from the previous homework\n",
        "\n",
        "\n",
        "def preprocess_data(train_df, test_df):\n",
        "    # Зберігаємо статистичні значення з тренувального набору даних\n",
        "    age_mean = train_df['Age'].mean()\n",
        "    embarked_mode = train_df['Embarked'].mode()[0]\n",
        "\n",
        "    # Заповнюємо пропуски у тренувальному та тестовому наборі однаковими значеннями\n",
        "    train_df['Age'] = train_df['Age'].fillna(age_mean)\n",
        "    test_df['Age'] = test_df['Age'].fillna(age_mean)\n",
        "\n",
        "    train_df['Embarked'] = train_df['Embarked'].fillna(embarked_mode)\n",
        "    test_df['Embarked'] = test_df['Embarked'].fillna(embarked_mode)\n",
        "\n",
        "    # Drop unnecessary columns that are not useful for modeling\n",
        "    train_df = train_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "    test_df = test_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "    # Застосовуємо one-hot encoding до обох наборів даних\n",
        "    train_df = pd.get_dummies(train_df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "    test_df = pd.get_dummies(test_df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Call this function before splitting data\n",
        "train_df, test_df = preprocess_data(train_df, test_df)\n",
        "\n",
        "X_train = train_df.drop('Survived', axis=1)  # Features\n",
        "y_train = train_df['Survived']  # Target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "n_splits = 5\n",
        "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "for train_index, val_index in stratified_kfold.split(train_df, train_df['Survived']):\n",
        "    X_train_fold, X_val_fold = train_df.iloc[train_index], train_df.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = train_df['Survived'].iloc[train_index], train_df['Survived'].iloc[val_index]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "Best Hyperparameters: {'estimator__max_depth': 10, 'estimator__min_samples_leaf': 2, 'estimator__min_samples_split': 5, 'max_features': 0.75, 'max_samples': 0.5, 'n_estimators': 100}\n",
            "Best Cross-Validation Accuracy: 0.837043238451689\n",
            "Training Accuracy: 0.8960674157303371\n"
          ]
        }
      ],
      "source": [
        "# define the bagging model (from sklearn)\n",
        "# define the hyperparameters grid\n",
        "# define the grid search with cross validation using previously defined validation method\n",
        "# train the model\n",
        "# print the best hyperparameters\n",
        "# print the best score on train and validation data, estimate the generalization error\n",
        "\n",
        "# Define the base estimator (DecisionTree)\n",
        "base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the BaggingClassifier model using the 'estimator' parameter\n",
        "bagging_model = BaggingClassifier(estimator=base_estimator, random_state=42)\n",
        "\n",
        "# Define the hyperparameters grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100],\n",
        "    'max_samples': [0.5],\n",
        "    'max_features': [0.75],\n",
        "    'estimator__max_depth': [10],\n",
        "    'estimator__min_samples_split': [5],\n",
        "    'estimator__min_samples_leaf': [2]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV with StratifiedKFold\n",
        "grid_search = GridSearchCV(estimator=bagging_model, \n",
        "                           param_grid=param_grid, \n",
        "                           scoring='accuracy', \n",
        "                           cv=stratified_kfold,  # From earlier\n",
        "                           n_jobs=-1, \n",
        "                           verbose=1)\n",
        "\n",
        "# Train the model with the grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and best score\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_bagging_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model on training data (just for reference)\n",
        "train_predictions = best_bagging_model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "print(f\"Training Accuracy: {train_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZSixk8wXjZJZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8156424581005587\n"
          ]
        }
      ],
      "source": [
        "# implement the custom bagging model\n",
        "\n",
        "class CustomBaggingClassifier:\n",
        "    def __init__(self, base_estimator, n_estimators=10, max_samples=1.0, max_features=1.0):\n",
        "        # Initialize with the base estimator, number of estimators, sample size, and feature size\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.max_features = max_features\n",
        "        self.estimators_ = []  # Will hold the trained base estimators\n",
        "        self.sample_indices_ = []  # Store indices of samples used for training\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        n_samples, n_features = X_train.shape\n",
        "        \n",
        "        # Calculate the number of samples and features to use for each base estimator\n",
        "        sample_size = int(self.max_samples * n_samples)\n",
        "        feature_size = int(self.max_features * n_features)\n",
        "        \n",
        "        # Train n_estimators base estimators\n",
        "        self.estimators_ = []  # Reset list of trained estimators\n",
        "        for i in range(self.n_estimators):\n",
        "            # 1. Draw max_samples samples from X_train with replacement\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=True)\n",
        "            # 2. Draw max_features features from X_train without replacement\n",
        "            feature_indices = np.random.choice(n_features, size=feature_size, replace=False)\n",
        "            \n",
        "            # 3. Train the estimator on the drawn samples and features using .iloc\n",
        "            X_sampled = X_train.iloc[sample_indices, feature_indices]\n",
        "            y_sampled = y_train.iloc[sample_indices]\n",
        "            \n",
        "            estimator = clone(self.base_estimator)\n",
        "            estimator.fit(X_sampled, y_sampled)\n",
        "            \n",
        "            # 4. Save the trained estimator and its corresponding features\n",
        "            self.estimators_.append((estimator, feature_indices))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        # Predict the label for each estimator and take the majority vote\n",
        "        predictions = np.array([estimator.predict(X_test.iloc[:, features]) for estimator, features in self.estimators_])\n",
        "        \n",
        "        # Perform majority voting across all base estimators\n",
        "        majority_votes = [Counter(preds).most_common(1)[0][0] for preds in predictions.T]\n",
        "        \n",
        "        return np.array(majority_votes)\n",
        "\n",
        "    def predict_proba(self, X_test):\n",
        "        # Predict the probabilities for each estimator and average the results\n",
        "        probas = np.array([estimator.predict_proba(X_test.iloc[:, features]) for estimator, features in self.estimators_])\n",
        "        return np.mean(probas, axis=0)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # Return the parameters of the CustomBaggingClassifier for grid search\n",
        "        return {\n",
        "            'base_estimator': self.base_estimator,\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'max_samples': self.max_samples,\n",
        "            'max_features': self.max_features\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        # Set the parameters of the CustomBaggingClassifier from a dictionary\n",
        "        for param, value in params.items():\n",
        "            setattr(self, param, value)\n",
        "        return self\n",
        "\n",
        "\n",
        "# Instantiate CustomBaggingClassifier\n",
        "custom_bagging = CustomBaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, max_samples=0.8, max_features=0.8)\n",
        "\n",
        "\n",
        "# Train the custom bagging classifier\n",
        "custom_bagging.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = custom_bagging.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7988826815642458\n"
          ]
        }
      ],
      "source": [
        "# Implement the custom bagging model\n",
        "class CustomBaggingClassifier:\n",
        "    def __init__(self, base_estimator, n_estimators=10, max_samples=1.0, max_features=1.0):\n",
        "        # Initialize with the base estimator, number of estimators, sample size, and feature size\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.max_features = max_features\n",
        "        self.estimators_ = []  # Will hold the trained base estimators\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        n_samples, n_features = X_train.shape\n",
        "        \n",
        "        # Calculate the number of samples and features to use for each base estimator\n",
        "        sample_size = int(self.max_samples * n_samples)\n",
        "        feature_size = int(self.max_features * n_features)\n",
        "        \n",
        "        # Train n_estimators base estimators\n",
        "        self.estimators_ = []  # Reset list of trained estimators\n",
        "        for i in range(self.n_estimators):\n",
        "            # 1. Draw max_samples samples from X_train with replacement\n",
        "            sample_indices = np.random.choice(n_samples, size=sample_size, replace=True)\n",
        "            # 2. Draw max_features features from X_train without replacement\n",
        "            feature_indices = np.random.choice(n_features, size=feature_size, replace=False)\n",
        "            \n",
        "            # 3. Train the estimator on the drawn samples and features using .iloc\n",
        "            X_sampled = X_train.iloc[sample_indices, feature_indices]\n",
        "            y_sampled = y_train.iloc[sample_indices]\n",
        "            \n",
        "            estimator = clone(self.base_estimator)\n",
        "            estimator.fit(X_sampled, y_sampled)\n",
        "            \n",
        "            # 4. Save the trained estimator and its corresponding features\n",
        "            self.estimators_.append((estimator, feature_indices))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X_test):\n",
        "        # Predict the probabilities for each estimator and average the results\n",
        "        probas = np.array([estimator.predict_proba(X_test.iloc[:, features]) for estimator, features in self.estimators_])\n",
        "        return np.mean(probas, axis=0)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        # Use predict_proba to get probabilities for class 0 and class 1\n",
        "        avg_probas = self.predict_proba(X_test)\n",
        "        \n",
        "        # Return 1 if the probability of class 1 is greater than or equal to 0.5, otherwise return 0\n",
        "        return (avg_probas[:, 1] >= 0.5).astype(int)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # Return the parameters of the CustomBaggingClassifier for grid search\n",
        "        return {\n",
        "            'base_estimator': self.base_estimator,\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'max_samples': self.max_samples,\n",
        "            'max_features': self.max_features\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        # Set the parameters of the CustomBaggingClassifier from a dictionary\n",
        "        for param, value in params.items():\n",
        "            setattr(self, param, value)\n",
        "        return self\n",
        "\n",
        "\n",
        "# Instantiate CustomBaggingClassifier\n",
        "custom_bagging = CustomBaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, max_samples=0.8, max_features=0.8)\n",
        "\n",
        "# Train the custom bagging classifier\n",
        "custom_bagging.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = custom_bagging.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "Best Hyperparameters: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Best Cross-Validation Accuracy: 0.8451007469713137\n"
          ]
        }
      ],
      "source": [
        "# define the random forest model \n",
        "# define the hyperparameters grid\n",
        "# define the grid search with cross validation using previously defined validation method\n",
        "# train the model\n",
        "# print the best hyperparameters\n",
        "# print the best score on train and validation data, estimate the generalization error\n",
        "\n",
        "# Load your dataset and preprocess it (assuming preprocess_data function exists)\n",
        "train_df = pd.read_csv('C:\\\\Users\\\\FILMINVASION\\\\Downloads\\\\ML2024\\\\train.csv')\n",
        "test_df = pd.read_csv('C:\\\\Users\\\\FILMINVASION\\\\Downloads\\\\ML2024\\\\test.csv')\n",
        "train_df, test_df = preprocess_data(train_df, test_df)\n",
        "\n",
        "# Define X and y (Features and Target)\n",
        "X = train_df.drop('Survived', axis=1)  # Features\n",
        "y = train_df['Survived']  # Target\n",
        "\n",
        "# Define StratifiedKFold Cross-Validation\n",
        "n_splits = 5\n",
        "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameters grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [None, 10],\n",
        "    'min_samples_split': [10],\n",
        "    'min_samples_leaf': [1],\n",
        "    'bootstrap': [True]\n",
        "}\n",
        "\n",
        "# Define the GridSearchCV with cross-validation using StratifiedKFold\n",
        "grid_search = GridSearchCV(estimator=rf_model, \n",
        "                           param_grid=param_grid, \n",
        "                           scoring='accuracy', \n",
        "                           cv=stratified_kfold, \n",
        "                           n_jobs=-1, \n",
        "                           verbose=1)\n",
        "\n",
        "# Train the model with Grid Search on the training data\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Get the best hyperparameters and print the best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DT Accuracy: 0.8960674157303371\n",
            "Custom Bagging Accuracy: 0.7988826815642458\n",
            "Best RF Accuracy: 0.8451007469713137\n"
          ]
        }
      ],
      "source": [
        "# compare the results of the three models from this homework and with DT from the previous homework\n",
        "# make a conclusion on which model is better and why\n",
        "# if your custom implementation is much worse than the sklearn one, try to improve it\n",
        "print(f\"DT Accuracy: {train_accuracy}\")\n",
        "\n",
        "print(f\"Custom Bagging Accuracy: {accuracy}\")\n",
        "\n",
        "print(f\"Best RF Accuracy: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load test data\n",
        "# do the same preprocessing as for train data\n",
        "\n",
        "# using retrained models make predictions on the test data for all new three models\n",
        "# save the predictions to a file\n",
        "# upload the predictions to Kaggle and make a submission\n",
        "# report the score you got and compare it with the score you got on the validation data\n",
        "# make a conclusion on how well the models generalizes"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Bagging homework.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
